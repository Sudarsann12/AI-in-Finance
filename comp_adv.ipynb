{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from PyPDF2 import PdfReader\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    filtered_tokens = [\n",
    "        lemmatizer.lemmatize(word) \n",
    "        for word in tokens if word.isalnum() and word not in stop_words\n",
    "    ]\n",
    "    return filtered_tokens\n",
    "\n",
    "def contains_keywords(text, keywords):\n",
    "    for keyword in keywords:\n",
    "        if re.search(r'\\b' + re.escape(keyword) + r'\\b', text, re.IGNORECASE):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def extract_competitive_advantage_section(pdf_path, start_page, end_page):\n",
    "    reader = PdfReader(pdf_path)\n",
    "    competitive_advantage_text = \"\"\n",
    "\n",
    "    for page_num in range(start_page - 1, end_page):  \n",
    "        page_text = reader.pages[page_num].extract_text()\n",
    "        if contains_keywords(page_text, keywords):\n",
    "            competitive_advantage_text += page_text + \"\\n\"\n",
    "    \n",
    "    return competitive_advantage_text\n",
    "\n",
    "keywords = [\n",
    "    \"competitive advantage\", \"industry position\", \"market leadership\", \n",
    "    \"dominance\", \"strategic position\", \"edge over competitors\", \n",
    "    \"superiority in market\", \"advantage over peers\"\n",
    "]\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "pdf_path = \"rhp.pdf\"\n",
    "start_page = int(input(\"Enter the start page: \"))\n",
    "end_page = int(input(\"Enter the end page: \"))\n",
    "\n",
    "competitive_advantage_text = extract_competitive_advantage_section(pdf_path, start_page, end_page)\n",
    "filtered_tokens = preprocess_text(competitive_advantage_text)\n",
    "\n",
    "\n",
    "print(\"Extracted Competitive Advantage Text (First 1000 characters):\")\n",
    "print(competitive_advantage_text[:1000])\n",
    "\n",
    "print(\"\\nPreprocessed Tokens (First 50 tokens):\")\n",
    "print(filtered_tokens[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load SpaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(competitive_advantage_text)\n",
    "\n",
    "advantage_keywords = [\n",
    "    \"better\", \"more\", \"less\", \"greater\", \"largest\", \"stronger\", \"leading\",\n",
    "    \"dominant\", \"innovative\", \"exclusive\", \"differentiated\", \"superior\",\n",
    "    \"advanced\", \"unique\", \"competitive edge\", \"edge over\", \"advantage in\",\n",
    "    \"compared to\", \"relative to\", \"versus\", \"market leader\", \"industry leader\"\n",
    "]\n",
    "\n",
    "comparative_phrases = []\n",
    "for sentence in doc.sents:\n",
    "    if any(keyword in sentence.text.lower() for keyword in advantage_keywords):\n",
    "        comparative_phrases.append(sentence.text)\n",
    "\n",
    "\n",
    "print(\"Extracted Comparative Phrases:\")\n",
    "for i, phrase in enumerate(comparative_phrases[:20], 1):  # Display the first 20 for readability\n",
    "    print(f\"{i}. {phrase}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "\n",
    "# FinBERT \n",
    "tokenizer = AutoTokenizer.from_pretrained(\"yiyanghkust/finbert-tone\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"yiyanghkust/finbert-tone\")\n",
    "\n",
    "# sentiment analysis pipeline\n",
    "finbert_classifier = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "comparative_sentiments = {}\n",
    "for phrase in comparative_phrases:\n",
    "    sentiment = finbert_classifier(phrase)\n",
    "    # Assign polarity score\n",
    "    if sentiment[0][\"label\"] == \"positive\":\n",
    "        polarity_score = sentiment[0][\"score\"]\n",
    "    elif sentiment[0][\"label\"] == \"negative\":\n",
    "        polarity_score = -sentiment[0][\"score\"]\n",
    "    else:  # Neutral case\n",
    "        polarity_score = 0\n",
    "    comparative_sentiments[phrase] = polarity_score\n",
    "\n",
    "print(\"Sentiment Scores (Improved):\", comparative_sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import numpy as np\n",
    "\n",
    "embedder = SentenceTransformer(\"all-mpnet-base-v2\")  # Upgraded model\n",
    "embeddings = embedder.encode(comparative_phrases)\n",
    "\n",
    "silhouette_scores = []\n",
    "potential_clusters = range(2, 11)  # Testing for 2 to 10 clusters\n",
    "for k in potential_clusters:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    labels = kmeans.fit_predict(embeddings)\n",
    "    score = silhouette_score(embeddings, labels)\n",
    "    silhouette_scores.append(score)\n",
    "\n",
    "optimal_clusters = potential_clusters[np.argmax(silhouette_scores)]\n",
    "\n",
    "# Perform Clustering\n",
    "clustering_model = KMeans(n_clusters=optimal_clusters, random_state=42)\n",
    "cluster_labels = clustering_model.fit_predict(embeddings)\n",
    "\n",
    "# Organize\n",
    "clustered_phrases = {i: [] for i in range(optimal_clusters)}\n",
    "for i, label in enumerate(cluster_labels):\n",
    "    clustered_phrases[label].append(comparative_phrases[i])\n",
    "\n",
    "print(f\"Optimal Number of Clusters: {optimal_clusters}\")\n",
    "print(\"Clustered Phrases:\", clustered_phrases)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "reduced_embeddings = pca.fit_transform(embeddings)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "for cluster_id in range(optimal_clusters):\n",
    "    cluster_points = reduced_embeddings[cluster_labels == cluster_id]\n",
    "    plt.scatter(cluster_points[:, 0], cluster_points[:, 1], label=f\"Cluster {cluster_id}\")\n",
    "plt.legend()\n",
    "plt.title(\"Comparative Phrases Clusters\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "cluster_scores = {}\n",
    "for cluster, phrases in clustered_phrases.items():\n",
    "    if phrases:\n",
    "        weights = [abs(comparative_sentiments[phrase]) for phrase in phrases]\n",
    "        weighted_scores = [comparative_sentiments[phrase] * weights[i] for i, phrase in enumerate(phrases)]\n",
    "        cluster_scores[cluster] = sum(weighted_scores) / sum(weights)\n",
    "    else:\n",
    "        cluster_scores[cluster] = 0\n",
    "\n",
    "min_score = min(cluster_scores.values())\n",
    "max_score = max(cluster_scores.values())\n",
    "normalized_scores = {\n",
    "    cluster: (score - min_score) / (max_score - min_score) if max_score != min_score else 0\n",
    "    for cluster, score in cluster_scores.items()\n",
    "}\n",
    "\n",
    "\n",
    "competitive_advantage_score = sum(normalized_scores.values())\n",
    "scaled_score = competitive_advantage_score * 100 / len(clustered_phrases)\n",
    "\n",
    "print(\"Cluster Scores (Weighted & Normalized):\", normalized_scores)\n",
    "print(\"Competitive Advantage Score (Scaled to 0-100):\", scaled_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
