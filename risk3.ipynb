{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/sudarsan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/sudarsan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "BertForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "Some weights of BertForMaskedLM were not initialized from the model checkpoint at yiyanghkust/finbert-tone and are newly initialized: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Word  Frequency\n",
      "0           operations        137\n",
      "1            financial        134\n",
      "2             partners        134\n",
      "3                india        130\n",
      "4               equity        121\n",
      "...                ...        ...\n",
      "2973          validity          1\n",
      "2974         fiduciary          1\n",
      "2975            duties          1\n",
      "2976  responsibilities          1\n",
      "2977         asserting          1\n",
      "\n",
      "[2978 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import re\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "model_name = 'yiyanghkust/finbert-tone'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "file_path = 'input_text.txt'  # Replace with your file path if different\n",
    "\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    text = file.read()\n",
    "\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "custom_stopwords = stop_words.union({\"company\", \"business\", \"include\", \"may\", \"could\", \"also\", \n",
    "                                     \"would\", \"page\", \"see\", \"us\", \"used\", \"result\", \"ensure\", \n",
    "                                     \"certain\", \"based\", \"account\", \"amounts\", \"objects\", \"continue\"})\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "   \n",
    "    text = re.sub(r'\\W', ' ', text.lower())\n",
    "    \n",
    "    words = word_tokenize(text)\n",
    "    filtered_words = [word for word in words if word not in custom_stopwords and len(word) > 1]\n",
    "    return filtered_words\n",
    "\n",
    "\n",
    "filtered_words = preprocess_text(text)\n",
    "\n",
    "\n",
    "word_freq = Counter(filtered_words)\n",
    "\n",
    "\n",
    "sorted_word_freq = dict(sorted(word_freq.items(), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "\n",
    "word_freq_df = pd.DataFrame(list(sorted_word_freq.items()), columns=['Word', 'Frequency'])\n",
    "print(word_freq_df)\n",
    "\n",
    "\n",
    "word_freq_df.to_csv('financial_word_frequencies.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "from collections import Counter\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel, pipeline\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "sentiment_analyzer = pipeline(\"sentiment-analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel\n",
    "model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_text(filename):\n",
    "    with open(filename, 'r') as file:\n",
    "        text = file.read()\n",
    "    return text\n",
    "\n",
    "\n",
    "def tokenize_and_count(text):\n",
    "    words = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "    word_counts = Counter(words)\n",
    "    return word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    outputs = model(**inputs)\n",
    "    embedding = outputs.last_hidden_state.mean(dim=1).squeeze().detach().numpy()\n",
    "    return embedding\n",
    "\n",
    "\n",
    "def extract_high_frequency_terms(word_counts, text, threshold=3):\n",
    "    high_freq_terms = {word: count for word, count in word_counts.items() if count >= threshold}\n",
    "    word_embeddings = {word: get_embedding(word) for word in high_freq_terms}\n",
    "    \n",
    "    doc_embedding = get_embedding(text)  \n",
    "    \n",
    "    return high_freq_terms, word_embeddings, doc_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_net_effect(text, terms, threshold=0):\n",
    "    term_effects = {}\n",
    "    sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', text)\n",
    "    \n",
    "    for term in terms:\n",
    "        term_sentiment = []\n",
    "        for sentence in sentences:\n",
    "            if term in sentence.lower():\n",
    "                sentiment = sentiment_analyzer(sentence)[0]\n",
    "                term_sentiment.append(sentiment['score'] if sentiment['label'] == \"POSITIVE\" else -sentiment['score'])\n",
    "\n",
    "\n",
    "        net_effect = sum(term_sentiment)\n",
    "        term_effects[term] = \"Positive\" if net_effect > threshold else \"Negative\" if net_effect < -threshold else \"Neutral\"\n",
    "\n",
    "    return term_effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def categorize_risks(term_effects, high_freq_terms, word_counts):\n",
    "    general_risks = []\n",
    "    specific_risks = []\n",
    "\n",
    "    for term, effect in term_effects.items():\n",
    "        if high_freq_terms[term] >= np.percentile(list(word_counts.values()), 75):\n",
    "            general_risks.append((term, effect))\n",
    "        else:\n",
    "            specific_risks.append((term, effect))\n",
    "    \n",
    "    return general_risks, specific_risks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 're' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 25>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mterm\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00meffect\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Run on an example text file\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m \u001b[43mprocess_risk_text_file\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_text.txt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36mprocess_risk_text_file\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_risk_text_file\u001b[39m(filename):\n\u001b[1;32m      3\u001b[0m     text \u001b[38;5;241m=\u001b[39m load_text(filename)\n\u001b[0;32m----> 4\u001b[0m     word_counts \u001b[38;5;241m=\u001b[39m \u001b[43mtokenize_and_count\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# Extract high-frequency terms\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     high_freq_terms, word_embeddings, doc_embedding \u001b[38;5;241m=\u001b[39m extract_high_frequency_terms(word_counts, text)\n",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36mtokenize_and_count\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokenize_and_count\u001b[39m(text):\n\u001b[0;32m----> 9\u001b[0m     words \u001b[38;5;241m=\u001b[39m \u001b[43mre\u001b[49m\u001b[38;5;241m.\u001b[39mfindall(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m, text\u001b[38;5;241m.\u001b[39mlower())\n\u001b[1;32m     10\u001b[0m     word_counts \u001b[38;5;241m=\u001b[39m Counter(words)\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m word_counts\n",
      "\u001b[0;31mNameError\u001b[0m: name 're' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "def process_risk_text_file(filename):\n",
    "    text = load_text(filename)\n",
    "    word_counts = tokenize_and_count(text)\n",
    "    \n",
    "    \n",
    "    high_freq_terms, word_embeddings, doc_embedding = extract_high_frequency_terms(word_counts, text)\n",
    "    \n",
    "    \n",
    "    term_effects = compute_net_effect(text, high_freq_terms)\n",
    "    \n",
    "   \n",
    "    general_risks, specific_risks = categorize_risks(term_effects, high_freq_terms, word_counts)\n",
    "    \n",
    "    \n",
    "    print(\"\\nGeneral Risks with Net Effect:\")\n",
    "    for term, effect in general_risks:\n",
    "        print(f\"{term}: {effect}\")\n",
    "    \n",
    "    print(\"\\nSpecific Risks with Net Effect:\")\n",
    "    for term, effect in specific_risks:\n",
    "        print(f\"{term}: {effect}\")\n",
    "\n",
    "# Run on an example text file\n",
    "process_risk_text_file(\"input_text.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "\n",
    "def process_risk_text_file(filename, output_csv=\"risk_analysis.csv\"):\n",
    "    text = load_text(filename)\n",
    "    word_counts = tokenize_and_count(text)\n",
    "    \n",
    "    \n",
    "    high_freq_terms, word_embeddings, doc_embedding = extract_high_frequency_terms(word_counts, text)\n",
    "    \n",
    "    \n",
    "    term_effects = compute_net_effect(text, high_freq_terms)\n",
    "    \n",
    "   \n",
    "    general_risks, specific_risks = categorize_risks(term_effects, high_freq_terms, word_counts)\n",
    "    \n",
    "    risk_data = []\n",
    "    for term, effect in general_risks:\n",
    "        risk_data.append({\"Risk Type\": \"General\", \"Term\": term, \"Net Effect\": effect})\n",
    "    for term, effect in specific_risks:\n",
    "        risk_data.append({\"Risk Type\": \"Specific\", \"Term\": term, \"Net Effect\": effect})\n",
    "    \n",
    "    with open(output_csv, mode='w', newline='') as csv_file:\n",
    "        writer = csv.DictWriter(csv_file, fieldnames=[\"Risk Type\", \"Term\", \"Net Effect\"])\n",
    "        writer.writeheader()\n",
    "        writer.writerows(risk_data)\n",
    "    \n",
    "    print(f\"Results have been saved to {output_csv}\")\n",
    "\n",
    "process_risk_text_file(\"input_text.txt\", \"risk_analysis.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
